# Dynamic Accelerator Slicer (DAS) Operator

Dynamic Accelerator Slicer (DAS) is an operator that dynamically partitions GPU accelerators in Kubernetes and OpenShift. It currently ships with a reference implementation for NVIDIA Multi-Instance GPU (MIG) and is designed to support additional technologies such as NVIDIA MPS or GPUs from other vendors.

## Table of Contents
- [Features](#features)
- [Getting Started](#getting-started)
  - [Emulated Mode](#emulated-mode)
  - [Running on OpenShift](#running-on-openshift)
  - [Operator Bundle Development](#operator-bundle-development)
- [Architecture](#architecture)
  - [MIG scheduler plugin](#mig-scheduler-plugin)
  - [AllocationClaim resource](#allocationclaim-resource)
- [Debugging](#debugging)
- [Running E2E tests](#running-e2e-tests)
- [Uninstalling](#uninstalling)
- [Contributing](#contributing)
- [License](#license)

## Features

- On-demand partitioning of GPUs via a custom Kubernetes operator.
- Scheduler integration that allocates NVIDIA MIG slices through a plugin located at [`pkg/scheduler/plugins/mig/mig.go`](pkg/scheduler/plugins/mig/mig.go).
- `AllocationClaim` custom resource to track slice reservations (`pkg/apis/dasoperator/v1alpha1/allocation_types.go`).
- Emulated mode to exercise the workflow without real hardware.

## Getting Started

### Kubernetes

The operator can be deployed on any Kubernetes cluster. To build and push the images run:

```bash
IMAGE_REGISTRY=<image_registry> IMAGE_TAG=<tag> make emulated-k8s   # no GPUs
```
e.g.
```
IMAGE_REGISTRY=quay.io/harpatil IMAGE_TAG=dev  make emulated-k8s
```

Apply one of the sample pods to verify the installation. For example, in emulated mode:

```bash
kubectl apply -f test/test-pod-emulated.yaml
```

### Running on OpenShift

Prerequisites are documented in [docs/nvidia-gpu-openshift.md](docs/nvidia-gpu-openshift.md). Once the prerequisites are met, you can deploy the operator with:

```bash
IMAGE_REGISTRY=<image_registry> IMAGE_TAG=<tag> make emulated-ocp   # no GPUs
IMAGE_REGISTRY=<image_registry> IMAGE_TAG=<tag> make gpu-ocp        # with GPUs
```
Please note that when deployed using `emulated-ocp`, the cert manager operator needs to be installed in the Openshift cluster using the OperatorHub before running the command.
e.g.
```
IMAGE_REGISTRY=quay.io/harpatil IMAGE_TAG=dev make gpu-ocp
```

If GPUs are present, test with:

```bash
kubectl apply -f test/test-pod.yaml
```

### Operator Bundle Development

1. Login into podman and have a repository created for the operator bundle.
2. Set `BUNDLE_IMAGE` to point to your repository and tag of choice.
3. Run `make bundle-generate` to generate the bundle manifests.
4. Run `make bundle-build` to build the `bundle-ocp.Dockerfile`.
5. Run `make bundle-push` to push the bundle image to your repository.
6. Run `make deploy-cert-manager` to install the OperatorGroup and Subscription.
7. Run `operator-sdk run bundle --namespace <namespace> ${BUNDLE_IMAGE}` to deploy the operator.

**Uisng a base CSV for bundle generation**

Running `generate bundle` is the first step to publishing an operator to a catalog
and deploying it with OLM. A CSV manifest is generated by collecting data from the
set of manifests passed to this command, such as CRDs, RBAC, etc., and applying
that data to a "base" CSV manifest.
The steps to provide a base CSV:
- create a base CSV file that contains the desired metadata, the base CSV file name can be
arbitrary, we can follow the convention `{operator-name}.base.clusterserviceverison.yaml`
- put the base CSV file in the `deploy` folder. This is the folder from which the
`generate bundle` command will collect the k8s manifests. Note that the base CSV
file can be placed inside a sub-directory within the `deploy` folder.
- make sure that the `metadata.name` of the base CSV is the same name as the
package name provided to the `generate bundle` command, otherwise the 
`generate bundle` command will ignore the base CSV and will generate on an empty CSV.


Layout of an example `deploy` folder:
```bash
$ tree deploy/
  deploy/
  ├── crds
  │   └── foo-operator.crd.yaml
  ├── base-csv
  │   └── foo-operator.base.clusterserviceversion.yaml
  ├── deployment.yaml
  ├── role.yaml
  ├── role_binding.yaml
  ├── service_account.yaml
  └── webhooks.yaml
```

The bundle generation command:
```bash
$ operator-sdk generate bundle --input-dir deploy --version 0.1.0 --output-dir=bundle --package foo-operator

```

The base CSV yaml:
```yaml
apiVersion: operators.coreos.com/v1alpha1
kind: ClusterServiceVersion
metadata:
  name: foo-operator.base
  annotations:
    alm-examples:
	# other annotaions can be placed here
spec:
  displayName: Instaslice
  version: 0.0.2
  apiservicedefinitions:
  customresourcedefinitions:
  install:
  installModes:
  - supported: false
    type: OwnNamespace
  - supported: false
    type: SingleNamespace
  - supported: false
    type: MultiNamespace
  - supported: true
    type: AllNamespaces
  maturity: alpha
  minKubeVersion: 1.16.0
  provider:
    name: Codeflare
    url: https://github.com/openshift/instaslice-operator
  relatedImages:
  keywords:
  - Foo
  links:
  - name: My Operator
    url: https://github.com/foo/bar
  maintainers:
  description:
  icon:
```

- There is no need to provide any permission, or deployment spec inside the base CSV.
- Note that the `metadata.name` of the base CSV has a prefix of `foo-operator.` which
adheres to the format `{package name}.`
- if there are multiple CSV files inside the deploy folder, the one encountered first 
in lexical order will be selected as the base CSV

The CSV generation details can be found by inspecting the bundle generation code here:
https://github.com/operator-framework/operator-sdk/blob/0eefc52889ff3dfe4af406038709e6c5ba7398e5/internal/generate/clusterserviceversion/clusterserviceversion.go#L148-L159


## Architecture

The diagram below summarizes how the operator components interact. Pods requesting GPU slices are mutated by a webhook to use the `mig.das.com` extended resource. The scheduler plugin tracks slice availability and creates `AllocationClaim` objects processed by the device plugin on each node.

![DAS Architecture](docs/images/arch.png)

### MIG scheduler plugin

The plugin integrates with the Kubernetes scheduler and runs through three framework phases:

* **Filter** – ensures the node is MIG capable and stages `AllocationClaim`s for suitable GPUs.
* **Score** – prefers nodes with the most free MIG slice slots after considering existing and staged claims.
* **PreBind** – promotes staged claims on the selected node to `created` and removes the rest.

Once promoted, the device plugin provisions the slices.

### AllocationClaim resource

`AllocationClaim` is a namespaced CRD that records which MIG slice will be prepared for a pod. Claims start in the `staged` state and transition to `created` once all requests are satisfied. Each claim stores the GPU UUID, slice position and pod reference.

Example:

```console
$ kubectl get allocationclaims -n das-operator
NAME                                          AGE
8835132e-8a7a-4766-a78f-0cb853d165a2-busy-0   61s
```

```console
$ kubectl get allocationclaims -n das-operator -o yaml
apiVersion: inference.redhat.com/v1alpha1
kind: AllocationClaim
...
```

## Emulated mode

When `emulatedMode` is enabled in the `DASOperator` custom resource, the operator publishes synthetic GPU capacity and skips NVML calls. This is handy for development and CI environments with no hardware.

## Debugging

All components run in the `das-operator` namespace:

```console
kubectl get pods -n das-operator
```

Inspect the active claims:

```console
kubectl get allocationclaims -n das-operator
```

On the node, verify that the CDI devices were created:

```console
ls -l /var/run/cdi/
```

Increase verbosity by editing the `DASOperator` resource and setting `operatorLogLevel` to `Debug` or `Trace`.

## Running E2E tests

A running cluster with a valid `KUBECONFIG` is required:

```console
make test-e2e
```

You can focus on specific tests:

```console
make test-e2e FOCUS="GPU slices"
```

## Uninstalling

Remove the deployed resources with:

```bash
make cleanup-k8s
```

## Contributing

Contributions are welcome! Please open issues or pull requests.

## License

This project is licensed under the Apache 2.0 License.
